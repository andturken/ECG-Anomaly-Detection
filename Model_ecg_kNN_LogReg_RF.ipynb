{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from importlib import reload"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data_all to memory\n",
      "  <class 'numpy.ndarray'>       (103504, 150)\n",
      "loaded annotations_all to memory\n",
      "  <class 'numpy.ndarray'>       (103504, 1)\n",
      "loaded onsets_all to memory\n",
      "  <class 'numpy.ndarray'>       (103504, 1)\n",
      "loaded pt_codes_all to memory\n",
      "  <class 'numpy.ndarray'>       (103504, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-processed ECG data matrix into memory\n",
    "\n",
    "def load_ecg_data_matrix(filename='data_ecg_pickle'):\n",
    "    '''\n",
    "    Load ECG data matrix into memory\n",
    "    Update global variable dict using keys read from pickle file\n",
    "    '''\n",
    "    import pickle\n",
    "\n",
    "    with open('data_ecg.pickle', 'rb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        data_dict = pickle.load(f)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "data_dict =  load_ecg_data_matrix(filename='data_ecg_pickle')\n",
    "\n",
    "for key in data_dict.keys():\n",
    "    globals()[key] = data_dict[key]\n",
    "    print('loaded ' + key + ' to memory')\n",
    "    v = globals()[key]\n",
    "    print( '  ' + str(type(v)) + '       ' + str(v.shape) )\n",
    "\n",
    "del data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "# flag potential ecg artifacts: excessive variability and large negative deflections\n",
    "# on individual ECG segments\n",
    "\n",
    "data_all_shape_before_remove = data_all.shape\n",
    "\n",
    "def remove_ecg_artifacts(data_all):\n",
    "    data_all = data_all.copy()\n",
    "    std = np.std(data_all, axis=1)\n",
    "    std_th = std > np.mean(std) + 4*np.std(std)\n",
    "    amp_max_neg = np.min(data_all, axis=1)\n",
    "    amp_th = amp_max_neg < np.mean(amp_max_neg) - 4 * np.std(amp_max_neg)\n",
    "    # amp_th.sum(), std_th.sum(), np.logical_and( amp_th , std_th).sum()\n",
    "\n",
    "    ix = np.logical_and(~std_th, ~amp_th)\n",
    "    data = data_all[ix,:]\n",
    "    annotations = annotations_all[ix]\n",
    "    onsets = onsets_all[ix]\n",
    "    pt_codes = pt_codes_all[ix]\n",
    "\n",
    "    n_removed = sum(np.logical_or(std_th, amp_th))\n",
    "    n_kept = sum(np.logical_and(~std_th, ~amp_th))\n",
    "    print( 'Flagged ' + str( n_removed ) + ' out of ' + str( len(std_th)) + ' ECG segments as artifacts ' )\n",
    "    return data_all"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# heart beat types\n",
    "# N = Normal\n",
    "# focus on A, L, R, V five most common heart beat anomaly types\n",
    "\n",
    "tN = np.where(annotations=='N')[0]\n",
    "tA = np.where(annotations=='A')[0]\n",
    "tL = np.where(annotations=='L')[0]\n",
    "tR = np.where(annotations=='R')[0]\n",
    "tV = np.where(annotations=='V')[0]\n",
    "\n",
    "#indices for normal ecg and five most common abnormal ecg types\n",
    "ix = np.concatenate([tN, tA, tL, tR, tV])\n",
    "\n",
    "\n",
    "data = data[ix,:]\n",
    "annotations = annotations[ix]\n",
    "onsets = onsets_all[ix]\n",
    "pt_codes = pt_codes_all[ix]\n",
    "\n",
    "labels_str = annotations\n",
    "labels = np.zeros((len(labels_str), 1))\n",
    "# for i, lab in enumerate(np.unique(labels_str)):\n",
    "#     labels[labels_str==lab] = i\n",
    "labels[labels_str=='N']=0\n",
    "labels[labels_str=='A']=1\n",
    "labels[labels_str=='L']=2\n",
    "labels[labels_str=='R']=3\n",
    "labels[labels_str=='V']=4\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# Training  and test datasets:\n",
    "\n",
    "# ECG data from 30 patients used for training (further split during cross-validation\n",
    "# and in-sample testing (data from same patients used for both training and testing performance)\n",
    "# ECG data from furhter 14 patients used for out-sample testing,\n",
    "# to assess model performance on entirely new data\n",
    "\n",
    "# pts_1 = [101, 106, 108, 109, 112, 114, 115, 116, 118, 119, 122, 124, 201, 203, 205, 207, 208, 209, 215, 220, 223, 230]\n",
    "# pts_2 = [100, 103, 105, 111, 113, 117, 121, 123, 200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232, 233, 234]\n",
    "pts_1 = [101, 106, 108, 109, 112, 114, 115, 116, 118, 119, 122, 124, 121, 123, 200,\n",
    "         201, 203, 205, 207, 208, 209, 215, 220, 223, 230, 228, 231, 232, 233, 234]\n",
    "pts_2 = [100, 103, 105, 111, 113, 117, 202, 210, 212, 213, 214, 219, 221, 222]\n",
    "\n",
    "ix_pts_1 = np.where( np.isin(pt_codes , pts_1))[0]\n",
    "ix_pts_2 = np.where( np.isin(pt_codes , pts_2))[0]\n",
    "\n",
    "data_pts_1, labels_pts_1, onsets_pts_1, pt_codes_pts_1 = \\\n",
    "    data[ix_pts_1,:], labels[ix_pts_1], onsets[ix_pts_1], pt_codes[ix_pts_1]\n",
    "\n",
    "data_pts_2, labels_pts_2, onsets_pts_2, pt_codes_pts_2 = \\\n",
    "    data[ix_pts_2,:], labels[ix_pts_2], onsets[ix_pts_2], pt_codes[ix_pts_2]\n",
    "\n",
    "\n",
    "ix_train = np.where( np.isin(pt_codes , pts_1))[0]\n",
    "ix_test_out  = np.where( np.isin(pt_codes , pts_2))[0]\n",
    "\n",
    "data_train, labels_train, onsets_train, pt_codes_train = \\\n",
    "    data[ix_train,:], labels[ix_train], onsets[ix_train], pt_codes[ix_train]\n",
    "\n",
    "data_test_out, labels_test_out, onsets_test_out, pt_codes_test_out = \\\n",
    "    data[ix_test_out,:], labels[ix_test_out], onsets[ix_test_out], pt_codes[ix_test_out]\n",
    "\n",
    "\n",
    "iX = list(range(data_train.shape[0]))\n",
    "X = data_train\n",
    "y = labels_train.ravel()\n",
    "\n",
    "iX_train,  iX_test, y_train, y_test = \\\n",
    "    train_test_split(iX, y, test_size=0.25, stratify=y, random_state=0)\n",
    "\n",
    "X_train, X_test = X[iX_train], X[iX_test]\n",
    "\n",
    "X_test_out = data_test_out\n",
    "y_test_out = labels_test_out\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from joblib import dump, load"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline - Random guess from multinomial distribution based on class probabilities\n",
      "expected f1 = 59.65\n"
     ]
    }
   ],
   "source": [
    "# Baseline Model\n",
    "\n",
    "# Random guess, assign class labels based on prior probability of each class\n",
    "# Simulate 5-fold CV, compute f1, weighted average of f1 scores for each class\n",
    "\n",
    "num_samples = X_train.shape[0]\n",
    "\n",
    "ix = np.arange(0,num_samples)\n",
    "\n",
    "\n",
    "\n",
    "n_20pc = num_samples // 5\n",
    "\n",
    "f1_cv = np.zeros((5,))\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    rnd = np.random.random(num_samples)\n",
    "    ix_cv = ix[np.argsort(rnd)]\n",
    "    ix_cv_train = ix_cv[n_20pc:]\n",
    "    ix_cv_test = ix_cv[:n_20pc]\n",
    "    y_cv_train = y_train[ix_cv_train]\n",
    "    y_cv_test = y_train[ix_cv_test]\n",
    "    p_vals = np.zeros((5,))\n",
    "    for j in range(5):\n",
    "        p_vals[j] = np.sum((y_cv_train==j)) / len(y_cv_train)\n",
    "    p_vals = p_vals / np.sum(p_vals)\n",
    "    # print(p_vals)\n",
    "    y_cv_pred = np.argmax(np.random.multinomial(1, p_vals, len(y_cv_test)), axis=1)\n",
    "\n",
    "    f1_cv[i] = f1_score(y_cv_test, y_cv_pred, average='weighted')\n",
    "\n",
    "\n",
    "f1_baseline = np.round(np.mean(f1_cv) * 100,2)\n",
    "\n",
    "\n",
    "print('Baseline - Random guess from multinomial distribution based on class probabilities')\n",
    "print('expected f1 = ' + str(f1_baseline))\n",
    "# expected f1 = 60.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model ECG abnormality with K Nearest Neighbors classifier\n",
    "from sklearn.neighbors  import  KNeighborsClassifier\n",
    "param_grid = { 'n_neighbors': [3,4,5,10,20]}\n",
    "clf = KNeighborsClassifier()\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_out_pred = clf.predict(X_test_out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "['Model_kNN.joblib']"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_kNN = clf\n",
    "dump(clf_kNN, 'Model_kNN'  + '.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing K Nearest Neighbors model\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "                     weights='uniform')\n",
      " \n",
      "kNN classification performance - Train-test on same group of individuals\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.99      1.00      1.00     12492\n",
      "           A       0.98      0.92      0.95       524\n",
      "           L       0.99      0.99      0.99       672\n",
      "           R       0.99      1.00      0.99      1338\n",
      "           V       0.98      0.97      0.98      1373\n",
      "\n",
      "    accuracy                           0.99     16399\n",
      "   macro avg       0.99      0.97      0.98     16399\n",
      "weighted avg       0.99      0.99      0.99     16399\n",
      "\n",
      " \n",
      "kNN classification performance - Train on group 1, test on group 2\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.85      0.98      0.91     24409\n",
      "           A       0.19      0.23      0.21       444\n",
      "           L       1.00      0.42      0.59      4073\n",
      "           R       0.33      0.04      0.07      1892\n",
      "           V       0.59      0.70      0.64      1086\n",
      "\n",
      "    accuracy                           0.83     31904\n",
      "   macro avg       0.59      0.47      0.48     31904\n",
      "weighted avg       0.82      0.83      0.80     31904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Best performing K Nearest Neighbors model\\n')\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print('kNN classification performance - Train-test on same group of individuals\\n')\n",
    "print(classification_report(y_test, y_test_pred,     target_names=['N','A','L','R','V']))\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print('kNN classification performance - Train on group 1, test on group 2\\n')\n",
    "print(classification_report(y_test_out, y_out_pred,     target_names=['N','A','L','R','V']))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best performing Logistic Regression model\n",
      "\n",
      "LogisticRegression(C=100, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=5000, multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='sag', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      " \n",
      "LogReg classification performance - Train-test on same group of individuals\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.99      0.88      0.93     12492\n",
      "           A       0.50      0.88      0.63       524\n",
      "           L       0.82      0.97      0.89       672\n",
      "           R       0.83      0.96      0.89      1338\n",
      "           V       0.63      0.91      0.74      1373\n",
      "\n",
      "    accuracy                           0.89     16399\n",
      "   macro avg       0.75      0.92      0.82     16399\n",
      "weighted avg       0.92      0.89      0.90     16399\n",
      "\n",
      " \n",
      "LogReg classification performance - Train on group 1, test on group 2\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.86      0.69      0.77     24409\n",
      "           A       0.04      0.20      0.07       444\n",
      "           L       0.52      0.15      0.23      4073\n",
      "           R       0.13      0.32      0.18      1892\n",
      "           V       0.18      0.71      0.29      1086\n",
      "\n",
      "    accuracy                           0.59     31904\n",
      "   macro avg       0.35      0.41      0.31     31904\n",
      "weighted avg       0.74      0.59      0.64     31904\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  25 | elapsed: 15.2min remaining: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed: 21.5min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#clf = LogisticRegression( C=100, penalty='l2', solver='saga', tol=0.01, class_weight='balanced')\n",
    "\n",
    "param_grid = { 'C': [ 0.01, 0.1, 1, 10, 100, ]}\n",
    "clf = LogisticRegression(class_weight='balanced', solver='sag', max_iter=5000)\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_out_pred = clf.predict(X_test_out)\n",
    "\n",
    "clf_LogReg = clf\n",
    "dump(clf_LogReg, 'Model_LogReg'  + '.joblib')\n",
    "\n",
    "print('Best performing Logistic Regression model\\n')\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print('LogReg classification performance - Train-test on same group of individuals\\n')\n",
    "print(classification_report(y_test, y_test_pred,     target_names=['N','A','L','R','V']))\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print('LogReg classification performance - Train on group 1, test on group 2\\n')\n",
    "print(classification_report(y_test_out, y_out_pred,     target_names=['N','A','L','R','V']))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best performing Balanced Random Forest model\n",
      "\n",
      "BalancedRandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
      "                               class_weight='balanced_subsample',\n",
      "                               criterion='gini', max_depth=None,\n",
      "                               max_features='log2', max_leaf_nodes=None,\n",
      "                               max_samples=None, min_impurity_decrease=0.0,\n",
      "                               min_samples_leaf=2, min_samples_split=2,\n",
      "                               min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                               n_jobs=None, oob_score=True, random_state=None,\n",
      "                               replacement=False, sampling_strategy='auto',\n",
      "                               verbose=0, warm_start=False)\n",
      " \n",
      "bRF classification performance - Train-test on same group of individuals\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.97      0.99     12492\n",
      "           A       0.81      0.95      0.87       524\n",
      "           L       0.99      0.99      0.99       672\n",
      "           R       0.99      0.99      0.99      1338\n",
      "           V       0.87      0.99      0.93      1373\n",
      "\n",
      "    accuracy                           0.98     16399\n",
      "   macro avg       0.93      0.98      0.95     16399\n",
      "weighted avg       0.98      0.98      0.98     16399\n",
      "\n",
      " \n",
      "bRF classification performance - Train on group 1, test on group 2\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.89      0.92      0.91     24409\n",
      "           A       0.08      0.45      0.13       444\n",
      "           L       1.00      0.37      0.54      4073\n",
      "           R       0.42      0.04      0.07      1892\n",
      "           V       0.35      0.86      0.50      1086\n",
      "\n",
      "    accuracy                           0.79     31904\n",
      "   macro avg       0.55      0.53      0.43     31904\n",
      "weighted avg       0.85      0.79      0.78     31904\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "/home/at/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 15.3min finished\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "param_grid = { 'n_estimators': [50, 100, 1000],\n",
    "               'max_features': [ 'sqrt', 'log2', None],\n",
    "                'class_weight': ['balanced', 'balanced_subsample']}\n",
    "clf = BalancedRandomForestClassifier(oob_score = True)\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_out_pred = clf.predict(X_test_out)\n",
    "\n",
    "clf_bRF = clf\n",
    "dump(clf_bRF, 'Model_LogReg'  + '.joblib')\n",
    "\n",
    "print('Best performing Balanced Random Forest model\\n')\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print('bRF classification performance - Train-test on same group of individuals\\n')\n",
    "print(classification_report(y_test, y_test_pred,     target_names=['N','A','L','R','V']))\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print('bRF classification performance - Train on group 1, test on group 2\\n')\n",
    "print(classification_report(y_test_out, y_out_pred,     target_names=['N','A','L','R','V']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pts_100 = [101, 106, 108, 109, 112, 114, 115, 116, 118, 119, 122, 124, 121, 123,\n",
    "#            100, 103, 105, 111, 113, 117]\n",
    "# pts_200 = [200, 201, 203, 205, 207, 208, 209, 215, 220, 223, 230, 228, 231, 232, 233, 234]\n",
    "#\n",
    "# pts_100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}